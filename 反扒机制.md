## 反扒机制

- User-Agent: 模拟一个浏览器访问，是否是一个浏览器
- IP访问次数：自己的一个路由，连接外网，同是一个ip访问一个网站。需要添加延时
- Cookie: cookie的限制，需要先登陆，使用用户名和密码，用来记录登陆状态，同一vip被不同的地方访问。爬去微博，需要进行登陆，拿到cookie，微博如果进行ip访问次数，使用多个ip,使用一个cookie.



### 反扒机制处理：

- User-agent:setting设置，在setting里面进行设置，找到“USER_AGENT=”进行设置
  - 动态更换User-agent:在Middlware进行request的修改，修改headers，设置proxy，
  - 动态的。
- 请求延时(delay)：setting设置
- 代理使用(proxy)：



from scrapy import cmdline

cmdline.execute("scrapy crawl xxx".split())

### 动态设置user-agent与代理

1、downloadermiddlewares；

2、编写user-agent/proxy middlewares类；

3、修改setting中DOWNLOADER_MIDDLEWARES;

-----

如何编写中间件？只是编写一个类，有下面方法中的一个：

MiddleWare

- process_request(self,request,spider)  处理request,实现这个类
- process_response(request,response,spider)  处理 response
- process_exception(request,exception,spider)处理异常



打开middlewares.py

创建

class DwMiddleWare(object):

​	def process_request(self,request,spider):

​	print("call process_request")

运行之后，发现并没有生效，我们需要在setting里进行添加，在setting里找到注销的DOWNLOADER_MIDDLEWARES 进行添加DwMiddleWare。

开始替换user_agent，DwMiddleWare类中添加__init__()方法。在开始导入：from fake_useragent imort FakeUserAgent

self.ua=FakeUserAgent()

sele.num =0

self.useragent = ""

放在__init__()方法中

if self.num%20==0

​	self.useragent=self.ua.random

request.header['User_Agent'] = self.useragent

print(request.headers['User_Agent']) 放在process_request中





